warnings()
print(modFit)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ ., data=training,method="nb")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
inTrain <- createDataPartition(segmentationOriginal,
p=0.7, list=FALSE)
head(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
rm(list=ls())
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
head(olive)
summary(olive$area)
str(olive)
table(olive$Area)
modelFit3<- train(Area~., data=olive,method="rpart")
result3<- predict(modelFit2,newdata)
result3<- predict(modelFit3,newdata)
newdata = as.data.frame(t(colMeans(olive)))
result3<- predict(modelFit3,newdata)
result3
rm(list=ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
set.seed(13234)
modelFit4 <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
result4_train<- missClass(trainSA$chd,predict(modelFit4,trainSA))
result4_test<- missClass(testSA$chd,predict(modelFit4,testSA))
result4_train
result4_test
rm(list=ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
modelFit5 <- train(y ~ ., method = "rf", data = vowel.train, prox = TRUE)
varImp(modelFit5$finalModel)
?varImp
require(caret); require(randomForest)
inTrain <- read.csv("pml-training.csv", sep=",", na.strings=c("NA","#DIV/0!",""))
inTest  <- read.csv("pml-testing.csv",  sep=",", na.strings=c("NA","#DIV/0!",""))
setwd("C:/Users/thewa/Documents/Self/Training/JohnsHopkinsDataScience/08 - Practical Machine Learning/Prog1/MachineLearning")
inTrain <- read.csv("pml-training.csv", sep=",", na.strings=c("NA","#DIV/0!",""))
inTest  <- read.csv("pml-testing.csv",  sep=",", na.strings=c("NA","#DIV/0!",""))
head(inTrain
)
str(inTrain)
rm(list=ls())
trainCsv <- read.csv("pml-training.csv", sep=",", na.strings=c("NA","#DIV/0!",""))
testCsv  <- read.csv("pml-testing.csv",  sep=",", na.strings=c("NA","#DIV/0!",""))
sum(complete.cases(trainCsv))
?unlist
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]
head(trainSet$classe)
str(trainSet$classe)
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet$classe <- as.factor(classe)
testing  <- testCsv [, colSums(is.na(testCsv))  != rnow(testCsv)]   #Remove if column is only NA
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet$classe <- as.factor(classe)
testing  <- testCsv [, colSums(is.na(testCsv))  != nrowow(testCsv)]   #Remove if column is only NA
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet$classe <- as.factor(classe)
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
set.seed(1073)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE, times=1)
training <- trainSet[ inTrain, ]
validate <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
predictRF <- predict(modelRF, validation)
confusionMatrix(validate$classe, predictRF)
predictRF <- predict(modelRF, validate)
confusionMatrix(validate$classe, predictRF)
dim(predictRF)
predictRF
predictRF$classe
str(predictRF)
str(predictRF)
controlRF <- trainControl(method="cv", 8)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
controlRF <- trainControl(method="cv", 5)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
rm(modelRF)
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
controlRF <- trainControl(method="cv", 10)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
rm(predictRF)
length(validate$classe)
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE, times=1)
training <- trainSet[ inTrain, ]
validate <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 10)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
predictRF <- predict(modelRF, validate)
predictRF <- predict(modelRF, newdata=validate)
predictRF
str(predictRF)
confusionMatrix(validate$classe, predictRF)
length(validate$classe)
accuracy <- postResample(predictRF, vaidate$classe)
str(modelRF)
plot(modelRF$finalModel)
modelNB  <- train(classe ~ ., data=training, method="lda")
modelLDA <- train(classe ~ ., data=training, method="nb")
pnb  <- predict(modelNB, validate)
plda <- predict(modelLDA, validate)
table(pnb, plda)
warnings()
plda
rm(list=ls())
trainCsv <- read.csv("pml-training.csv", sep=",", na.strings=c("NA","#DIV/0!",""))
testCsv  <- read.csv("pml-testing.csv",  sep=",", na.strings=c("NA","#DIV/0!",""))
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE, times=1)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 10)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
predictRF <- predict(modelRF, newdata=validatn)
confusionMatrix(validatn$classe, predictRF)
str(validatn$classe)
length(validatn$classe)
?predict()
requre(MASS)
modelNB  <- train(classe ~ ., data=training, method="lda")
modelLDA <- train(classe ~ ., data=training, method="nb")
modelGBM <- train(classe ~ ., data=training, method="gbm")
modelRP  <- train(classe ~ ., data=training, method="rpart")
pnb  <- predict(modelNB, validatn)
plda <- predict(modelLDA, validatn)
table(pnb, plda)
modelGBM
modelGBM$finalModel
modelRP
modelRP  <- train(classe ~ ., data=training, method="rpart", trControl=controlRF)
modelRP
varImpPlot(modelRF)
str(modelRF)
predictRF <- predict.train(modelRF, newdata=validatn)
confusionMatrix(validatn$classe, predictRF)
predictRF
require(rattle)
modelRP
print(modelRF)
plot(modelRF$finalModel)
plot(modelRF)
importance(modelRF)
varImpRF <- varImp(modelRF, scale=FALSE)
varImpRF
corrPlot <- cor(trainig[, -length(names(training))])
corrplot(corrPlot, method="color")
corrPlot <- cor(trainig[, -length(names(training))])
corrplot(corrPlot, method="color")
corrPlot <- cor(training[, -length(names(training))])
corrplot(corrPlot, method="color")
plot(varImpRF)
plot(varImpRF, top=20)
modelRP
print(modelRF$finalModel)
print(modelRP$finalModel)
plot(modelRP$finalModel)
controlRP <- trainControl(method="cv", 10)
modelRP  <- train(classe ~ ., data=training, method="rpart")
controlRP <- trainControl(method="cv", 6)
modelRP  <- train(classe ~ ., data=training, method="rpart")
plot(modelRP$finalModel)
print(modelRP$finalModel)
modelRF
plot(modelRF, log="y")
plot(modelRF$finalModel, log="y")
plot(modelRF$finalModel)
legend("top", colnames(modelRF$err.rate),col=1:4,cex=0.8,fill=1:4)
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(modelRF$err.rate),col=1:4,cex=0.8,fill=1:4)
legend("topright", legend=unique(modelRF$classification), col=unique(as.numeric(modelRF$classification)), pch=19)
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("topright", legend=unique(modelRF$classification), col=unique(as.numeric(modelRF$classification)), pch=19)
fancyRpartPlot(modelRP$finalModel)
modelRF
modelRF$finalModel
str(modelRF$finalModel)
?rpart
modelRP1 <- rpart(classe ~ ., data=training)
modelRP1
fancyRpartPlot(modelRP1)
pRP1 <- predict(modelRP1, validatn)
pRP1
modelRP1
str(modelRP1)
varImp(modelRP1)
varImp(modelRP1, top=20)
varImpRP1 <- varImp(modelRP1)
plot(varimprp1, top=10)
plot(varImpRP1, top=10)
varImpRP1 <- varImp(modelRP1, scale=FALSE)
varImpRP1 <- varImp(modelRP1, scale=FALSE)
plot(varImpRP1)
plot(varImpRP1)
varImpRP1
varImpRF
table(training$x)
summary(training$X)
head(training$X)
head(training)
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet <- trainSet[, -c(1,3,4,5,6,7)]                             #Drop the row number
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
testing  <- testing [, -c(1,3,4,5,6,7)]                             #Drop the row number
head(training)
head(trainSet)
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 10)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=300)
modelRF
controlRP <- trainControl(method="cv", 10)
modelRP  <- train(classe ~ ., data=training, method="rpart")
modelRP
?cforest
library(party)
cforest(classe ~ ., data=training)
modelR
getTree(modelRF)
modelRF1<- randomForest(classe ~ ., data=training)
str(training$classe)
modelrf
nzv <- nearZeroVar(trainSet)
nzv
trainSet <- trainSet[, -nzv]
testing  <- testing [, -nzv]
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=200)
modelRF1<- randomForest(classe ~ ., data=training)
modelRF
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet <- trainSet[, -c(1,3,4,5,6,7)]                             #Drop the row number
trainSet <- trainSet[, sapply(trainSet, is.numeric)]
#nzv <- nearZeroVar(trainSet)
#trainSet <- trainSet[, -nzv]
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
testing  <- testing [, -c(1,3,4,5,6,7)]                             #Drop the row number
testing  <- testing [, sapply(testing, is.numeric)]
#testing  <- testing [, -nzv]
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=200)
modelRF
sparse <- sapply(colnames(trainSet), function(x) if( (sum(is.na(trainSet[, x]))/nrow(trainSet))>0.50 ) {return(TRUE)} else{return(FALSE)} )
trainSet <- trainset[,!sparse]
sparse <- sapply(colnames(trainSet), function(x) if( (sum(is.na(trainSet[, x]))/nrow(trainSet))>0.50 ) {return(TRUE)} else{return(FALSE)} )
trainSet <- trainSet[,!sparse]
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet <- trainSet[, -(1:7)]                                      #Drop columns that cannot be predictors (timestamp, row number)
#trainSet <- trainSet[, sapply(trainSet, is.numeric)]
sparse <- sapply(colnames(trainSet), function(x) if( (sum(is.na(trainSet[, x]))/nrow(trainSet))>0.50 ) {return(TRUE)} else{return(FALSE)} )
trainSet <- trainset[,!sparse]
#nzv <- nearZeroVar(trainSet)
#trainSet <- trainSet[, -nzv]
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
testing  <- testing [, -(1:7)]                                      #columns that cannot be predictors
testing  <- testing [, !sparse]
#testing  <- testing [, sapply(testing, is.numeric)]
#testing  <- testing [, -nzv]
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet <- trainSet[, -(1:7)]                                      #Drop columns that cannot be predictors (timestamp, row number)
#trainSet <- trainSet[, sapply(trainSet, is.numeric)]
sparse <- sapply(colnames(trainSet), function(x) if( (sum(is.na(trainSet[, x]))/nrow(trainSet))>0.50 ) {return(TRUE)} else{return(FALSE)} )
trainSet <- trainSet[,!sparse]
#nzv <- nearZeroVar(trainSet)
#trainSet <- trainSet[, -nzv]
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
testing  <- testing [, -(1:7)]                                      #columns that cannot be predictors
testing  <- testing [, !sparse]
#testing  <- testing [, sapply(testing, is.numeric)]
#testing  <- testing [, -nzv]
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
controlRF <- trainControl(method="cv", 6)
modelRF <- train(classe ~ ., data=training, method="rf", trControl=controlRF, ntree=200)
modelRF
predictRF <- predict.train(modelRF, newdata=validatn)
confusionMatrix(validatn$classe, predictRF)
sum(complete.cases(trainCsv))
trainSet <- trainCsv[, colSums(is.na(trainCsv)) != nrow(trainCsv)]  #Remove if column is only NA
trainSet <- trainSet[, -(1:7)]                                      #Drop columns that cannot be predictors (timestamp, row number)
#trainSet <- trainSet[, sapply(trainSet, is.numeric)]
trainSet <- trainSet[, colSums(is.na(trainSet)) < nrow(trainSet) * 0.5]
#sparse <- sapply(colnames(trainSet), function(x) if( (sum(is.na(trainSet[, x]))/nrow(trainSet))>0.50 ) {return(TRUE)} else{return(FALSE)} )
#trainSet <- trainSet[,!sparse]
#nzv <- nearZeroVar(trainSet)
#trainSet <- trainSet[, -nzv]
testing  <- testCsv [, colSums(is.na(testCsv))  != nrow(testCsv)]   #Remove if column is only NA
testing  <- testing [, -(1:7)]                                      #columns that cannot be predictors
testing  <- testing [, !sparse]
#testing  <- testing [, sapply(testing, is.numeric)]
#testing  <- testing [, -nzv]
print(modelRF$finalModel)
varImpRF <- varImp(modelRF, scale=FALSE)
varImpRF
plot(varImpRF, top=20)
requre(MASS)
modelNB  <- train(classe ~ ., data=training, method="lda")
modelLDA <- train(classe ~ ., data=training, method="nb")
modelGBM <- train(classe ~ ., data=training, method="gbm")
controlRP <- trainControl(method="cv", 10)
modelRP  <- train(classe ~ ., data=training, method="rpart")
pRP1 <- predict(modelRP1, validatn)
pnb  <- predict(modelNB, validatn)
plda <- predict(modelLDA, validatn)
table(pnb, plda)
modelGBM
modelRP
fancyRpartPlot(modelRP)
fancyRpartPlot(modelRP$finalModel)
print(modelRF$finalModel)
sum(complete.cases(training))
head(training)
modelXGB <- train(classe ~ ., data=training, method="xgb")
library(xgboost)
modelXGB <- xgboost(classe ~ ., data=training)
equalPredictions <- (pnb == plda)
qplot(roll_belt, pitchforearm, colour=equalPredictions,data=validatn)
qplot(roll_belt, pitch_forearm, colour=equalPredictions,data=validatn)
table(pnb, plda)
confusionMatrix(validatn$classe, predictRF)
pRF <- predict.train(modelRF, newdata=validatn)
confusionMatrix(validatn$classe, predictRF)
equalPredictions <- (validatn$classe == pRF)
qplot(roll_belt, pitch_forearm, colour=equalPredictions,data=validatn)
?qplot
qplot(roll_belt, pitch_forearm, colour=classe,data=validatn)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point(alpha=0.5)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point(alpha=0.5,shape=equalPredictions)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point(shape=equalPredictions)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe, shape=equalPredictions))+ geom_point(alpha=0.5)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe, shape=equalPredictions))+ geom_point(alpha=0.5)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point(alpha=0.1)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()
head(equalPredictions)
eq <- as.numeric(equalPredictions)
eq
neq <- (as.numeric(!equalPredictions)*5)
neq
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()+ geom_point(size=neq)
rm(neq)
neq <- validatn[!equalPredictions, ]
neq
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=3,shape=4)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=5,shape=4)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=10,shape=4)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=classe))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=10,shape=0)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=predictRF))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=10,shape=0)
ggplot(data=validatn, aes(x=roll_belt, y=pitch_forearm, colour=predictRF))+ geom_point()+ geom_point(data=neq, aes(x=roll_belt, y=pitch_forearm, colour=classe), size=7,shape=0)
max(modelRF$accuracy)
max(modelRF$Accuracy)
max(modelRF$results$Accuracy)
confusionMatrix(validatn$classe, predictRF)
cmRF <- confusionMatrix(validatn$classe, predictRF)
str(cmRF)
plot(cmRF)
print(cmRF)
?postResample
postResample(validatn$classe, predictRF)
oose <- 1 - as.numeric(confusionMatrix(validatn$classe, predictRF)$overall[1])
oose
as.numeric(confusionMatrix(validatn$classe, predictRF)$overall[1])
max(modelRF$results$Accuracy)
cmRF$overall
rm(list=ls())
trainCsv <- read.csv("pml-training.csv", sep=",", na.strings=c("NA","#DIV/0!",""))
testCsv  <- read.csv("pml-testing.csv",  sep=",", na.strings=c("NA","#DIV/0!",""))
trainSet <- trainSet[, -(1:7)]                                            #Drop columns that cannot be predictors (timestamp, row number)
trainSet <- trainSet[, colSums(is.na(trainSet)) < nrow(trainSet) * 0.5]   #Drop columns with 50% NA or more
testing  <- testing [, -(1:7)]                                            #columns that cannot be predictors
testing  <- testing [, colSums(is.na(testing))  < nrow(testing) * 0.5]    #Drop columns with 50% NA or more
trainSet <- trainCsv[, -(1:7)]                                            #Drop columns that cannot be predictors (timestamp, row number)
trainSet <- trainSet[, colSums(is.na(trainSet)) < nrow(trainSet) * 0.5]   #Drop columns with 50% NA or more
testing  <- testCsv [, -(1:7)]                                            #Drop columns that cannot be predictors (timestamp, row number)
testing  <- testing [, colSums(is.na(testing))  < nrow(testing) * 0.5]    #Drop columns with 50% NA or more
set.seed(173)
inTrain <- createDataPartition(trainSet$classe, p=0.70, list=FALSE)
training <- trainSet[ inTrain, ]
validatn <- trainSet[-inTrain, ]
tcontrol <- trainControl(method="cv", number=6, verboseIter = FALSE)
modelRF  <- train(classe ~ ., data=training, method="rf",  trControl=tcontrol, ntree=200)
modelLDA <- train(classe ~ ., data=training, method="lda", trControl=tcontrol)
modelNB  <- train(classe ~ ., data=training, method="nb" , trControl=tcontrol)
modelGBM <- train(classe ~ ., data=training, method="gbm", trControl=tcontrol, verbose=FALSE)
modelRP  <- train(classe ~ ., data=training, method="rpart",trControl=tcontrol)
warnings()
pRF  <- predict(modelRF,  validatn)
pLDA <- predict(modelLDA, validatn)
pNB  <- predict(modelNB,  validatn)
pGBM <- precict(modelGBM, validatn)
pRP  <- predict(modelRP,  validatn)
pNB  <- predict(modelNB,  validatn)
pGBM <- predict(modelGBM, validatn)
cmRF <- confusionMatrix(validatn$classe, pRF)
cmRF <- confusionMatrix(validatn$classe, pLDA)
cmRF <- confusionMatrix(validatn$classe, pNB)
cmRF <- confusionMatrix(validatn$classe, pGBM)
cmRF <- confusionMatrix(validatn$classe, pRP)
table(pnb, plda)
cmRF  <- confusionMatrix(validatn$classe, pRF)
cmLDA <- confusionMatrix(validatn$classe, pLDA)
cmNB  <- confusionMatrix(validatn$classe, pNB)
cmGBM <- confusionMatrix(validatn$classe, pGBM)
cmRR  <- confusionMatrix(validatn$classe, pRP)
table(pnb, plda)
table(pNB, pLDA)
cmRP  <- confusionMatrix(validatn$classe, pRP)
Model <- c("Random forest", "Linear discriminant analysis","Naive Bayes","Gradient boosting machine","Rpart tree")
TrainAccuracy <- c(max(modelRF$results$Accuracy), max(modelLDA$results$Accuracy), max(modelNB$results$Accuracy), max(modelGBM$results$Accuracy),  max(modelRP$results$Accuracy))
ValidationAccuracy <- c(cmFR$overall[1], cmLDA$overall[1], cmNB$overall[1], cmGBM$overall[1], cmRP$overall[1])
OutOfSampleErr <- 1 - ValidationAccuracy
metrics <- cbind(Model, TrainAccuracy, ValidationAccuracy, OutOfSampleErr)
print(metrics)
Model <- c("Random forest", "Linear discriminant analysis","Naive Bayes","Gradient boosting machine","Rpart tree")
TrainAccuracy <- c(max(modelRF$results$Accuracy), max(modelLDA$results$Accuracy), max(modelNB$results$Accuracy), max(modelGBM$results$Accuracy),  max(modelRP$results$Accuracy))
ValidationAccuracy <- c(cmRF$overall[1], cmLDA$overall[1], cmNB$overall[1], cmGBM$overall[1], cmRP$overall[1])
OutOfSampleErr <- 1 - ValidationAccuracy
metrics <- cbind(Model, TrainAccuracy, ValidationAccuracy, OutOfSampleErr)
print(metrics)
str(metrics)
summary(metrics)
metrics
Model
?cbind
metrics <- cbind(Model, TrainAccuracy, ValidationAccuracy, OutOfSampleErr, make.row.names=FALSE)
metrics
str(Model)
print(metrics, row.names=FALSE)
str(metrics)
dim(metrics)
metrics <- data.frame(Model, TrainAccuracy, ValidationAccuracy, OutOfSampleErr)
metrics
print(metrics, row.names=FALSE)
